# Large AI Model Empowered Multimodal Semantic Communications
## Authors：Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You
## Paper：https://arxiv.org/abs/2309.01249
## Home page：[https://github.com/jiangfeibo/MultimodalSC](https://jiangfeibo.github.io/MultimodalSC/)
## Displayed video 1
https://github.com/jiangfeibo/MultimodalSC/assets/67264710/8d018280-b0a8-43e7-9825-101dba51bc3e
### Explanation of video 1：
  - Model：MobileVLM
  - Video content：
  
    Q：what did you see？

    
    A：in the image,a person is holding a book,possibly a textbook,with a flower design on the cover. the book is open and appears to be in a foreign language.the person is also holding a hand,possibly to hold the book or to show the book to someone.
## Displayed video 2
https://github.com/jiangfeibo/MultimodalSC/assets/67264710/7564d320-b041-4bb3-969d-817e418659e4
### Explanation of video 2：
  - Model：qwen1.6b
  - Pre filling rate：253.35token/s
  - Decoding rate：9.04token/s
  - Video content：

    Q：喂！你好
  
  
    A：你好！有什么我能帮助你的吗？
  
    
    Q：你能做些什么呢？
  
    
    A：我可以回答问题，提供定义，解释和建议，将文本从一种语言翻译成另一种语言，总结文本，生成文本，写故事，分析感情，提供建议，开发算法，编写代码以及任何其他基于语言的任务
## Displayed video 3
https://github.com/jiangfeibo/MultimodalSC/assets/67264710/f9f10be8-8a36-4983-91a4-3c927191845d
### Explanation of video 3：


    The video demonstrates the end-to-end facial video transmission framework we proposed. During transmission, text is used in place of video. FunASR and Bert-VITS2 are used for ASR and TTS, respectively, and finally, Sadtalker is used to reconstruct the video.



